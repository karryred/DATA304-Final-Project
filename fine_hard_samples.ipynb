{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcf68a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import math\n",
    "from collections import defaultdict\n",
    "\n",
    "# --- 1. 설정 ---\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "MAX_LEN = 256\n",
    "BATCH_SIZE = 32\n",
    "NUM_CLASSES = 531\n",
    "TOP_K_SAMPLES = 1000  # 추출할 샘플 개수\n",
    "\n",
    "# 경로\n",
    "BASE_DIR = \"Amazon_products\"\n",
    "TEST_CORPUS_PATH = os.path.join(BASE_DIR, \"test/test_corpus.txt\")\n",
    "CLASSES_PATH = os.path.join(BASE_DIR, \"classes.txt\")\n",
    "\n",
    "# 모델 경로\n",
    "PATH_BERT_MODEL = \"saved_model_gnn/best_model_gnn.pt\"\n",
    "PATH_DEBERTA_MODEL = \"saved_model_deberta_gnn/best_model_deberta.pt\"\n",
    "OUTPUT_CSV = \"hard_samples_for_llm.csv\"\n",
    "\n",
    "# --- 2. 모델 클래스 정의 ---\n",
    "class GraphConvolution(nn.Module):\n",
    "    def __init__(self, in_features, out_features, bias=True):\n",
    "        super(GraphConvolution, self).__init__()\n",
    "        self.weight = nn.Parameter(torch.FloatTensor(in_features, out_features))\n",
    "        if bias: self.bias = nn.Parameter(torch.FloatTensor(out_features))\n",
    "        else: self.register_parameter('bias', None)\n",
    "        self.reset_parameters()\n",
    "    def reset_parameters(self):\n",
    "        stdv = 1. / math.sqrt(self.weight.size(1))\n",
    "        self.weight.data.uniform_(-stdv, stdv)\n",
    "        if self.bias is not None: self.bias.data.uniform_(-stdv, stdv)\n",
    "    def forward(self, input, adj):\n",
    "        support = torch.mm(input, self.weight)\n",
    "        output = torch.mm(adj, support)\n",
    "        return output + self.bias if self.bias is not None else output\n",
    "\n",
    "class BertGCN(nn.Module):\n",
    "    def __init__(self, model_name, num_classes, hidden_dim, adj_matrix):\n",
    "        super(BertGCN, self).__init__()\n",
    "        self.bert = AutoModel.from_pretrained(model_name)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.label_embedding = nn.Parameter(torch.FloatTensor(num_classes, hidden_dim))\n",
    "        self.gcn = GraphConvolution(hidden_dim, hidden_dim)\n",
    "        self.adj_matrix = adj_matrix\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        doc_embedding = self.dropout(outputs.pooler_output)\n",
    "        refined_label_embedding = torch.tanh(self.gcn(self.label_embedding, self.adj_matrix))\n",
    "        return torch.mm(doc_embedding, refined_label_embedding.t())\n",
    "\n",
    "class DebertaGCN(nn.Module):\n",
    "    def __init__(self, model_name, num_classes, hidden_dim, adj_matrix):\n",
    "        super(DebertaGCN, self).__init__()\n",
    "        self.encoder = AutoModel.from_pretrained(model_name)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.label_embedding = nn.Parameter(torch.FloatTensor(num_classes, hidden_dim))\n",
    "        self.gcn = GraphConvolution(hidden_dim, hidden_dim)\n",
    "        self.adj_matrix = adj_matrix\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.encoder(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        doc_embedding = self.dropout(outputs.last_hidden_state[:, 0, :])\n",
    "        refined_label_embedding = torch.tanh(self.gcn(self.label_embedding, self.adj_matrix))\n",
    "        return torch.mm(doc_embedding, refined_label_embedding.t())\n",
    "\n",
    "# --- 3. 유틸리티 ---\n",
    "def load_test_data():\n",
    "    pids, texts = [], []\n",
    "    with open(TEST_CORPUS_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split(\"\\t\", 1)\n",
    "            if len(parts) == 2:\n",
    "                pids.append(parts[0])\n",
    "                texts.append(parts[1])\n",
    "    return pids, texts\n",
    "\n",
    "def load_class_names(path):\n",
    "    id2name = {}\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split(\"\\t\", 1)\n",
    "            if len(parts) == 2:\n",
    "                id2name[int(parts[0])] = parts[1].strip()\n",
    "    return id2name\n",
    "\n",
    "class TestDataset(Dataset):\n",
    "    def __init__(self, texts, tokenizer, max_len):\n",
    "        self.texts = texts\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "    def __len__(self): return len(self.texts)\n",
    "    def __getitem__(self, item):\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            str(self.texts[item]), add_special_tokens=True, max_length=self.max_len,\n",
    "            return_token_type_ids=False, padding='max_length', truncation=True,\n",
    "            return_attention_mask=True, return_tensors='pt'\n",
    "        )\n",
    "        return {'input_ids': encoding['input_ids'].flatten(), 'attention_mask': encoding['attention_mask'].flatten()}\n",
    "\n",
    "# --- 4. 메인 실행 ---\n",
    "def find_hard_samples():\n",
    "    print(\"1. Loading Data...\")\n",
    "    test_pids, test_texts = load_test_data()\n",
    "    id2name = load_class_names(CLASSES_PATH)\n",
    "    \n",
    "    # 각각의 토크나이저 로드 (use_fast=False 필수)\n",
    "    tokenizer_bert = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "    tokenizer_deberta = AutoTokenizer.from_pretrained(\"microsoft/deberta-v3-base\", use_fast=False)\n",
    "    \n",
    "    loader_bert = DataLoader(TestDataset(test_texts, tokenizer_bert, MAX_LEN), batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
    "    loader_deberta = DataLoader(TestDataset(test_texts, tokenizer_deberta, MAX_LEN), batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
    "\n",
    "    print(\"2. Loading Models...\")\n",
    "    model_bert = torch.load(PATH_BERT_MODEL, map_location=DEVICE, weights_only=False)\n",
    "    model_bert.to(DEVICE).eval()\n",
    "    \n",
    "    model_deberta = torch.load(PATH_DEBERTA_MODEL, map_location=DEVICE, weights_only=False)\n",
    "    model_deberta.to(DEVICE).eval()\n",
    "\n",
    "    results = []\n",
    "    print(\"3. Calculating Uncertainty (Entropy)...\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, (batch_b, batch_d) in enumerate(tqdm(zip(loader_bert, loader_deberta), total=len(loader_bert))):\n",
    "            # BERT 입력\n",
    "            ids_b = batch_b['input_ids'].to(DEVICE)\n",
    "            mask_b = batch_b['attention_mask'].to(DEVICE)\n",
    "            \n",
    "            # DeBERTa 입력\n",
    "            ids_d = batch_d['input_ids'].to(DEVICE)\n",
    "            mask_d = batch_d['attention_mask'].to(DEVICE)\n",
    "\n",
    "            # 앙상블 예측\n",
    "            logits_b = model_bert(ids_b, mask_b)\n",
    "            logits_d = model_deberta(ids_d, mask_d)\n",
    "            \n",
    "            # Soft Voting (확률 평균)\n",
    "            probs = (torch.sigmoid(logits_b) * 0.4) + (torch.sigmoid(logits_d) * 0.6)\n",
    "            \n",
    "            # Entropy 계산: -sum(p * log(p))\n",
    "            # (p가 0이면 log가 무한대가 되므로 1e-9를 더해줌)\n",
    "            entropy = -torch.sum(probs * torch.log(probs + 1e-9), dim=1)\n",
    "            \n",
    "            batch_entropy = entropy.cpu().numpy()\n",
    "            batch_probs = probs.cpu().numpy()\n",
    "            \n",
    "            start_idx = i * BATCH_SIZE\n",
    "            for j, ent in enumerate(batch_entropy):\n",
    "                pid = test_pids[start_idx + j]\n",
    "                text = test_texts[start_idx + j]\n",
    "                \n",
    "                # 모델이 생각하는 상위 10개 후보 (LLM에게 힌트로 주기 위함)\n",
    "                top10_idx = batch_probs[j].argsort()[-10:][::-1]\n",
    "                candidates = [f\"{idx}: {id2name.get(idx, 'Unknown')}\" for idx in top10_idx]\n",
    "                \n",
    "                results.append({\n",
    "                    \"pid\": pid,\n",
    "                    \"text\": text,\n",
    "                    \"entropy\": ent,\n",
    "                    \"candidates\": \" | \".join(candidates)\n",
    "                })\n",
    "\n",
    "    # Entropy 높은 순으로 정렬하여 상위 1000개 추출\n",
    "    df = pd.DataFrame(results)\n",
    "    df = df.sort_values(by=\"entropy\", ascending=False)\n",
    "    \n",
    "    hard_samples = df.head(TOP_K_SAMPLES)\n",
    "    hard_samples.to_csv(OUTPUT_CSV, index=False)\n",
    "    \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    find_hard_samples()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
