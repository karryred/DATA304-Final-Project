{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "d5c3bc0a",
        "outputId": "822d668a-e3bf-436c-f2dd-3cc691da3c85"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting Hierarchy-Aware Training...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/1659 [00:01<?, ?it/s]\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 159\u001b[39m\n\u001b[32m    156\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mTraining Complete.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    158\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m159\u001b[39m     \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 149\u001b[39m, in \u001b[36mtrain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    147\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m tqdm(train_loader):\n\u001b[32m    148\u001b[39m     optimizer.zero_grad()\n\u001b[32m--> \u001b[39m\u001b[32m149\u001b[39m     logits = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43minput_ids\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDEVICE\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mattention_mask\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDEVICE\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    150\u001b[39m     loss = criterion(logits, batch[\u001b[33m'\u001b[39m\u001b[33mtargets\u001b[39m\u001b[33m'\u001b[39m].to(DEVICE))\n\u001b[32m    151\u001b[39m     masked_loss = (loss * batch[\u001b[33m'\u001b[39m\u001b[33mmasks\u001b[39m\u001b[33m'\u001b[39m].to(DEVICE)).sum() / batch[\u001b[33m'\u001b[39m\u001b[33mmasks\u001b[39m\u001b[33m'\u001b[39m].to(DEVICE).sum()\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python314\\site-packages\\torch\\nn\\modules\\module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python314\\site-packages\\torch\\nn\\modules\\module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 75\u001b[39m, in \u001b[36mDebertaGCN.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask)\u001b[39m\n\u001b[32m     74\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, input_ids, attention_mask):\n\u001b[32m---> \u001b[39m\u001b[32m75\u001b[39m     outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     76\u001b[39m     doc_embedding = \u001b[38;5;28mself\u001b[39m.dropout(outputs.last_hidden_state[:, \u001b[32m0\u001b[39m, :])\n\u001b[32m     77\u001b[39m     refined_label_emb = torch.tanh(\u001b[38;5;28mself\u001b[39m.gcn(\u001b[38;5;28mself\u001b[39m.label_embedding, \u001b[38;5;28mself\u001b[39m.adj_matrix))\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python314\\site-packages\\torch\\nn\\modules\\module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python314\\site-packages\\torch\\nn\\modules\\module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python314\\site-packages\\transformers\\models\\deberta_v2\\modeling_deberta_v2.py:784\u001b[39m, in \u001b[36mDebertaV2Model.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, token_type_ids, position_ids, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[39m\n\u001b[32m    774\u001b[39m     token_type_ids = torch.zeros(input_shape, dtype=torch.long, device=device)\n\u001b[32m    776\u001b[39m embedding_output = \u001b[38;5;28mself\u001b[39m.embeddings(\n\u001b[32m    777\u001b[39m     input_ids=input_ids,\n\u001b[32m    778\u001b[39m     token_type_ids=token_type_ids,\n\u001b[32m   (...)\u001b[39m\u001b[32m    781\u001b[39m     inputs_embeds=inputs_embeds,\n\u001b[32m    782\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m784\u001b[39m encoder_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    785\u001b[39m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    786\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    787\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    788\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    789\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    790\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    791\u001b[39m encoded_layers = encoder_outputs[\u001b[32m1\u001b[39m]\n\u001b[32m    793\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.z_steps > \u001b[32m1\u001b[39m:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python314\\site-packages\\torch\\nn\\modules\\module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python314\\site-packages\\torch\\nn\\modules\\module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python314\\site-packages\\transformers\\models\\deberta_v2\\modeling_deberta_v2.py:657\u001b[39m, in \u001b[36mDebertaV2Encoder.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, output_hidden_states, output_attentions, query_states, relative_pos, return_dict)\u001b[39m\n\u001b[32m    655\u001b[39m rel_embeddings = \u001b[38;5;28mself\u001b[39m.get_rel_embedding()\n\u001b[32m    656\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, layer_module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m.layer):\n\u001b[32m--> \u001b[39m\u001b[32m657\u001b[39m     output_states, attn_weights = \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    658\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnext_kv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    659\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    660\u001b[39m \u001b[43m        \u001b[49m\u001b[43mquery_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquery_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    661\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrelative_pos\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrelative_pos\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    662\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrel_embeddings\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrel_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    663\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    664\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    666\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n\u001b[32m    667\u001b[39m         all_attentions = all_attentions + (attn_weights,)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python314\\site-packages\\transformers\\modeling_layers.py:94\u001b[39m, in \u001b[36mGradientCheckpointingLayer.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     91\u001b[39m         logger.warning_once(message)\n\u001b[32m     93\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._gradient_checkpointing_func(partial(\u001b[38;5;28msuper\u001b[39m().\u001b[34m__call__\u001b[39m, **kwargs), *args)\n\u001b[32m---> \u001b[39m\u001b[32m94\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python314\\site-packages\\torch\\nn\\modules\\module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python314\\site-packages\\torch\\nn\\modules\\module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python314\\site-packages\\transformers\\models\\deberta_v2\\modeling_deberta_v2.py:436\u001b[39m, in \u001b[36mDebertaV2Layer.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, query_states, relative_pos, rel_embeddings, output_attentions)\u001b[39m\n\u001b[32m    427\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\n\u001b[32m    428\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    429\u001b[39m     hidden_states,\n\u001b[32m   (...)\u001b[39m\u001b[32m    434\u001b[39m     output_attentions: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    435\u001b[39m ) -> \u001b[38;5;28mtuple\u001b[39m[torch.Tensor, Optional[torch.Tensor]]:\n\u001b[32m--> \u001b[39m\u001b[32m436\u001b[39m     attention_output, att_matrix = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    437\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    438\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    439\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    440\u001b[39m \u001b[43m        \u001b[49m\u001b[43mquery_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquery_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    441\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrelative_pos\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrelative_pos\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    442\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrel_embeddings\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrel_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    443\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    444\u001b[39m     intermediate_output = \u001b[38;5;28mself\u001b[39m.intermediate(attention_output)\n\u001b[32m    445\u001b[39m     layer_output = \u001b[38;5;28mself\u001b[39m.output(intermediate_output, attention_output)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python314\\site-packages\\torch\\nn\\modules\\module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python314\\site-packages\\torch\\nn\\modules\\module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python314\\site-packages\\transformers\\models\\deberta_v2\\modeling_deberta_v2.py:369\u001b[39m, in \u001b[36mDebertaV2Attention.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, output_attentions, query_states, relative_pos, rel_embeddings)\u001b[39m\n\u001b[32m    360\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\n\u001b[32m    361\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    362\u001b[39m     hidden_states,\n\u001b[32m   (...)\u001b[39m\u001b[32m    367\u001b[39m     rel_embeddings=\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    368\u001b[39m ) -> \u001b[38;5;28mtuple\u001b[39m[torch.Tensor, Optional[torch.Tensor]]:\n\u001b[32m--> \u001b[39m\u001b[32m369\u001b[39m     self_output, att_matrix = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mself\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    370\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    371\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    372\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    373\u001b[39m \u001b[43m        \u001b[49m\u001b[43mquery_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquery_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    374\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrelative_pos\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrelative_pos\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    375\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrel_embeddings\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrel_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    376\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    377\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m query_states \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    378\u001b[39m         query_states = hidden_states\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python314\\site-packages\\torch\\nn\\modules\\module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python314\\site-packages\\torch\\nn\\modules\\module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python314\\site-packages\\transformers\\models\\deberta_v2\\modeling_deberta_v2.py:250\u001b[39m, in \u001b[36mDisentangledSelfAttention.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, output_attentions, query_states, relative_pos, rel_embeddings)\u001b[39m\n\u001b[32m    248\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.relative_attention:\n\u001b[32m    249\u001b[39m     rel_embeddings = \u001b[38;5;28mself\u001b[39m.pos_dropout(rel_embeddings)\n\u001b[32m--> \u001b[39m\u001b[32m250\u001b[39m     rel_att = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdisentangled_attention_bias\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    251\u001b[39m \u001b[43m        \u001b[49m\u001b[43mquery_layer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey_layer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrelative_pos\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrel_embeddings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale_factor\u001b[49m\n\u001b[32m    252\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    254\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m rel_att \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    255\u001b[39m     attention_scores = attention_scores + rel_att\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python314\\site-packages\\transformers\\models\\deberta_v2\\modeling_deberta_v2.py:321\u001b[39m, in \u001b[36mDisentangledSelfAttention.disentangled_attention_bias\u001b[39m\u001b[34m(self, query_layer, key_layer, relative_pos, rel_embeddings, scale_factor)\u001b[39m\n\u001b[32m    319\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mc2p\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.pos_att_type:\n\u001b[32m    320\u001b[39m     scale = scaled_size_sqrt(pos_key_layer, scale_factor)\n\u001b[32m--> \u001b[39m\u001b[32m321\u001b[39m     c2p_att = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbmm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery_layer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpos_key_layer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtranspose\u001b[49m\u001b[43m(\u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    322\u001b[39m     c2p_pos = torch.clamp(relative_pos + att_span, \u001b[32m0\u001b[39m, att_span * \u001b[32m2\u001b[39m - \u001b[32m1\u001b[39m)\n\u001b[32m    323\u001b[39m     c2p_att = torch.gather(\n\u001b[32m    324\u001b[39m         c2p_att,\n\u001b[32m    325\u001b[39m         dim=-\u001b[32m1\u001b[39m,\n\u001b[32m    326\u001b[39m         index=c2p_pos.squeeze(\u001b[32m0\u001b[39m).expand([query_layer.size(\u001b[32m0\u001b[39m), query_layer.size(\u001b[32m1\u001b[39m), relative_pos.size(-\u001b[32m1\u001b[39m)]),\n\u001b[32m    327\u001b[39m     )\n",
            "\u001b[31mKeyboardInterrupt\u001b[39m: "
          ]
        }
      ],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import math\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import AutoTokenizer, AutoModel, get_linear_schedule_with_warmup\n",
        "from torch.optim import AdamW\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tqdm import tqdm\n",
        "from collections import defaultdict\n",
        "\n",
        "# --- 1. 환경 및 경로 설정 ---\n",
        "def seed_everything(seed=42):\n",
        "    import random\n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "\n",
        "seed_everything(42)\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "MODEL_NAME = \"microsoft/deberta-v3-base\"\n",
        "BASE_DIR = os.path.abspath(\"..\") \n",
        "OUTPUT_DIR = os.path.join(BASE_DIR, \"after\")\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "# 하이퍼파라미터\n",
        "BATCH_SIZE = 16\n",
        "EPOCHS = 5\n",
        "LEARNING_RATE = 1e-5\n",
        "MAX_LEN = 256\n",
        "NUM_CLASSES = 531\n",
        "HIDDEN_DIM = 768\n",
        "\n",
        "# --- 2. 계층 구조 데이터 로더 ---\n",
        "def load_hierarchy_maps(path):\n",
        "    parents = defaultdict(list)\n",
        "    children = defaultdict(list)\n",
        "    with open(path, \"r\") as f:\n",
        "        for line in f:\n",
        "            p, c = map(int, line.split())\n",
        "            parents[c].append(p)\n",
        "            children[p].append(c)\n",
        "    return parents, children\n",
        "\n",
        "# --- 3. 모델 정의 (DeBERTa-GCN) ---\n",
        "class GraphConvolution(nn.Module):\n",
        "    def __init__(self, in_features, out_features):\n",
        "        super().__init__()\n",
        "        self.weight = nn.Parameter(torch.FloatTensor(in_features, out_features))\n",
        "        self.bias = nn.Parameter(torch.FloatTensor(out_features))\n",
        "        nn.init.xavier_uniform_(self.weight)\n",
        "        nn.init.zeros_(self.bias)\n",
        "\n",
        "    def forward(self, input, adj):\n",
        "        support = torch.mm(input, self.weight)\n",
        "        return torch.mm(adj, support) + self.bias\n",
        "\n",
        "class DebertaGCN(nn.Module):\n",
        "    def __init__(self, model_name, num_classes, hidden_dim, adj_matrix):\n",
        "        super().__init__()\n",
        "        self.encoder = AutoModel.from_pretrained(model_name)\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "        self.label_embedding = nn.Parameter(torch.empty(num_classes, hidden_dim))\n",
        "        nn.init.xavier_uniform_(self.label_embedding)\n",
        "        self.gcn = GraphConvolution(hidden_dim, hidden_dim)\n",
        "        self.adj_matrix = adj_matrix\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        outputs = self.encoder(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        doc_embedding = self.dropout(outputs.last_hidden_state[:, 0, :])\n",
        "        refined_label_emb = torch.tanh(self.gcn(self.label_embedding, self.adj_matrix))\n",
        "        return torch.mm(doc_embedding, refined_label_emb.t())\n",
        "\n",
        "# --- 4. 계층 인지 데이터셋 ---\n",
        "class HATDataset(Dataset):\n",
        "    def __init__(self, texts, silver_labels, parents_map, children_map, tokenizer, max_len):\n",
        "        self.texts, self.labels = texts, silver_labels\n",
        "        self.parents, self.children = parents_map, children_map\n",
        "        self.tokenizer, self.max_len = tokenizer, max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, item):\n",
        "        core_cids = [int(x) for x in str(self.labels[item]).split(\",\") if x]\n",
        "        target = torch.zeros(NUM_CLASSES)\n",
        "        mask = torch.ones(NUM_CLASSES) # Loss 가중치 (1: 학습, 0: 무시)\n",
        "\n",
        "        pos_set = set(core_cids)\n",
        "        ignore_set = set()\n",
        "        for cid in core_cids:\n",
        "            \n",
        "            curr = cid\n",
        "            while curr in self.parents:\n",
        "                p_list = self.parents[curr]\n",
        "                pos_set.update(p_list)\n",
        "                curr = p_list[0] if p_list else -1\n",
        "            \n",
        "            if cid in self.children:\n",
        "                ignore_set.update(self.children[cid])\n",
        "\n",
        "        target[list(pos_set)] = 1.0\n",
        "        mask[list(ignore_set)] = 0.0 \n",
        "\n",
        "        enc = self.tokenizer.encode_plus(str(self.texts[item]), max_length=self.max_len, padding='max_length', truncation=True, return_tensors='pt')\n",
        "        return {'input_ids': enc['input_ids'].flatten(), 'attention_mask': enc['attention_mask'].flatten(), 'targets': target, 'masks': mask}\n",
        "\n",
        "# --- 5. 학습 루프 ---\n",
        "def train():\n",
        "    hierarchy_path = os.path.join(BASE_DIR, \"Amazon_products\", \"class_hierarchy.txt\") # Corrected path\n",
        "    parents, children = load_hierarchy_maps(hierarchy_path)\n",
        "\n",
        "    train_csv_path = os.path.join(BASE_DIR, \"final\", \"train_round_2.csv\") # Corrected path\n",
        "    df = pd.read_csv(train_csv_path)\n",
        "\n",
        "    # 텍스트 데이터 로드\n",
        "    train_corpus_path = os.path.join(BASE_DIR, \"Amazon_products\", \"train\", \"train_corpus.txt\") # Corrected path\n",
        "    pid2text = {}\n",
        "    with open(train_corpus_path, \"r\") as f:\n",
        "        for line in f:\n",
        "            p, t = line.strip().split(\"\\t\", 1)\n",
        "            pid2text[int(p)] = t\n",
        "    df['text'] = df['pid'].map(pid2text)\n",
        "    df = df.dropna()\n",
        "\n",
        "    train_df, val_df = train_test_split(df, test_size=0.1, random_state=42)\n",
        "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=False)\n",
        "\n",
        "    train_loader = DataLoader(HATDataset(train_df.text.to_numpy(), train_df.labels.to_numpy(), parents, children, tokenizer, MAX_LEN), batch_size=BATCH_SIZE, shuffle=True)\n",
        "    val_loader = DataLoader(HATDataset(val_df.text.to_numpy(), val_df.labels.to_numpy(), parents, children, tokenizer, MAX_LEN), batch_size=BATCH_SIZE)\n",
        "\n",
        "    # 인접 행렬 및 모델 초기화\n",
        "    adj = torch.eye(NUM_CLASSES).to(DEVICE) # 단순화를 위해 I 사용\n",
        "    model = DebertaGCN(MODEL_NAME, NUM_CLASSES, HIDDEN_DIM, adj).to(DEVICE)\n",
        "    optimizer = AdamW(model.parameters(), lr=LEARNING_RATE)\n",
        "    criterion = nn.BCEWithLogitsLoss(reduction='none') # 마스킹을 위해 none 설정\n",
        "\n",
        "    print(\"Starting Hierarchy-Aware Training...\")\n",
        "    for epoch in range(EPOCHS):\n",
        "        model.train()\n",
        "        for batch in tqdm(train_loader):\n",
        "            optimizer.zero_grad()\n",
        "            logits = model(batch['input_ids'].to(DEVICE), batch['attention_mask'].to(DEVICE))\n",
        "            loss = criterion(logits, batch['targets'].to(DEVICE))\n",
        "            masked_loss = (loss * batch['masks'].to(DEVICE)).sum() / batch['masks'].to(DEVICE).sum()\n",
        "            masked_loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        torch.save(model, os.path.join(OUTPUT_DIR, \"best_deberta_hat.pt\"))\n",
        "    print(\"Training Complete.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    train()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SGzO_CMes-U6",
        "outputId": "ad741a3c-17ef-4a96-d3bc-c3b68d96acf7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1. Loading Models and Tokenizers...\n",
            "✅ BERT model loaded successfully\n",
            "❌ Error loading DeBERTa model: Ran out of input\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import csv\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import AutoTokenizer, AutoModel, BertModel\n",
        "from collections import defaultdict\n",
        "\n",
        "# --- 설정 및 경로 ---\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "BASE_DIR = os.path.join(os.path.abspath(\"..\"), \"Amazon_products\")\n",
        "MAX_LEN = 256\n",
        "BATCH_SIZE = 32\n",
        "NUM_CLASSES = 531\n",
        "\n",
        "# 모델 경로\n",
        "PATH_BERT = os.path.join(os.path.abspath(\"..\"), \"final\", \"saved_model_gnn\", \"best_model_gnn.pt\")\n",
        "PATH_DEBERTA = os.path.join(os.path.abspath(\"..\"), \"after\", \"best_deberta_hat.pt\")\n",
        "FINAL_CSV = \"1220.csv\"\n",
        "\n",
        "# --- 1. 모델 클래스 정의 ---\n",
        "\n",
        "class GraphConvolution(nn.Module):\n",
        "    def __init__(self, in_features, out_features):\n",
        "        super().__init__()\n",
        "        self.weight = nn.Parameter(torch.FloatTensor(in_features, out_features))\n",
        "        self.bias = nn.Parameter(torch.FloatTensor(out_features))\n",
        "        nn.init.xavier_uniform_(self.weight)\n",
        "        nn.init.zeros_(self.bias)\n",
        "\n",
        "    def forward(self, input, adj):\n",
        "        support = torch.mm(input, self.weight)\n",
        "        return torch.mm(adj, support) + self.bias\n",
        "\n",
        "class DebertaGCN(nn.Module):\n",
        "    \"\"\"DeBERTa 기반 GNN 모델\"\"\"\n",
        "    def __init__(self, model_name, num_classes, hidden_dim, adj_matrix):\n",
        "        super().__init__()\n",
        "        self.encoder = AutoModel.from_pretrained(model_name)\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "        self.label_embedding = nn.Parameter(torch.empty(num_classes, hidden_dim))\n",
        "        nn.init.xavier_uniform_(self.label_embedding)\n",
        "        self.gcn = GraphConvolution(hidden_dim, hidden_dim)\n",
        "        self.adj_matrix = adj_matrix\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        outputs = self.encoder(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        # DeBERTa-v3는 pooler_output이 없는 경우가 많아 last_hidden_state의 [CLS] 사용\n",
        "        doc_embedding = self.dropout(outputs.last_hidden_state[:, 0, :])\n",
        "        refined_label_emb = torch.tanh(self.gcn(self.label_embedding, self.adj_matrix))\n",
        "        return torch.mm(doc_embedding, refined_label_emb.t())\n",
        "\n",
        "class BertGCN(nn.Module):\n",
        "    \"\"\"\n",
        "    BERT 기반 GNN 모델 - 에러 해결을 위해 구조 수정\n",
        "    (DebertaGCN과 동일하게 라벨 임베딩과의 행렬 곱 방식으로 변경)\n",
        "    \"\"\"\n",
        "    def __init__(self, num_labels, hidden_dim=768, adj_matrix=None):\n",
        "        super(BertGCN, self).__init__()\n",
        "        self.bert = BertModel.from_pretrained(\"bert-base-uncased\")\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "        self.label_embedding = nn.Parameter(torch.empty(num_labels, hidden_dim))\n",
        "        nn.init.xavier_uniform_(self.label_embedding)\n",
        "        self.gcn = GraphConvolution(hidden_dim, hidden_dim)\n",
        "        self.adj_matrix = adj_matrix \n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        # BERT의 [CLS] 토큰 임베딩 (일관성을 위해 last_hidden_state 사용)\n",
        "        doc_embedding = self.dropout(outputs.last_hidden_state[:, 0, :])\n",
        "\n",
        "        # 모델에 내장된 adj_matrix 사용 (없을 경우를 대비해 예외 처리)\n",
        "        adj = getattr(self, 'adj_matrix', None)\n",
        "        if adj is None:\n",
        "            # 추론 시 adj_matrix가 없으면 GCN 없이 임베딩만 사용하거나 에러 방지\n",
        "            refined_label_emb = self.label_embedding\n",
        "        else:\n",
        "            refined_label_emb = torch.tanh(self.gcn(self.label_embedding, adj))\n",
        "\n",
        "        return torch.mm(doc_embedding, refined_label_emb.t())\n",
        "\n",
        "# --- 2. 헬퍼 함수 ---\n",
        "\n",
        "def patch_model_config(model):\n",
        "    \"\"\"라이브러리 호환성 문제 해결\"\"\"\n",
        "    target_models = []\n",
        "    if hasattr(model, 'bert'): target_models.append(model.bert)\n",
        "    if hasattr(model, 'encoder'): target_models.append(model.encoder)\n",
        "\n",
        "    for m in target_models:\n",
        "        if hasattr(m, 'config'):\n",
        "            for attr in ['_output_attentions', '_output_hidden_states', 'output_attentions', 'output_hidden_states']:\n",
        "                if not hasattr(m.config, attr):\n",
        "                    setattr(m.config, attr, False)\n",
        "    return model\n",
        "\n",
        "def load_hierarchy_maps():\n",
        "    parents = defaultdict(list)\n",
        "    hierarchy_path = os.path.join(BASE_DIR, \"class_hierarchy.txt\")\n",
        "    if not os.path.exists(hierarchy_path):\n",
        "        return parents\n",
        "    with open(hierarchy_path, \"r\") as f:\n",
        "        for line in f:\n",
        "            parts = line.split()\n",
        "            if len(parts) == 2:\n",
        "                p, c = map(int, parts)\n",
        "                parents[c].append(p)\n",
        "    return parents\n",
        "\n",
        "# --- 3. 실행부 ---\n",
        "\n",
        "def run_ensemble_inference():\n",
        "    print(\"1. Loading Models and Tokenizers...\")\n",
        "\n",
        "    try:\n",
        "        model_bert = torch.load(PATH_BERT, map_location=DEVICE, weights_only=False)\n",
        "        model_bert = patch_model_config(model_bert)\n",
        "        model_bert = model_bert.to(DEVICE).eval()\n",
        "        print(\"✅ BERT model loaded successfully\")\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Error loading BERT model: {e}\")\n",
        "        return\n",
        "\n",
        "    try:\n",
        "        model_deberta = torch.load(PATH_DEBERTA, map_location=DEVICE, weights_only=False)\n",
        "        model_deberta = patch_model_config(model_deberta)\n",
        "        model_deberta = model_deberta.to(DEVICE).eval()\n",
        "        print(\"✅ DeBERTa model loaded successfully\")\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Error loading DeBERTa model: {e}\")\n",
        "        return\n",
        "\n",
        "    parents_map = load_hierarchy_maps()\n",
        "    tokenizer_b = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "    tokenizer_d = AutoTokenizer.from_pretrained(\"microsoft/deberta-v3-base\", use_fast=False)\n",
        "\n",
        "    # 데이터 로드\n",
        "    test_pids, test_texts = [], []\n",
        "    test_corpus_path = os.path.join(BASE_DIR, \"test/test_corpus.txt\")\n",
        "    with open(test_corpus_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            if \"\\t\" in line:\n",
        "                p, t = line.strip().split(\"\\t\", 1)\n",
        "                test_pids.append(p)\n",
        "                test_texts.append(t)\n",
        "\n",
        "    print(f\"2. Starting Inference (Total: {len(test_texts)} samples)...\")\n",
        "    all_preds = []\n",
        "\n",
        "    for i in tqdm(range(0, len(test_texts), BATCH_SIZE)):\n",
        "        batch_texts = test_texts[i:i+BATCH_SIZE]\n",
        "\n",
        "        # BERT 예측\n",
        "        enc_b = tokenizer_b(batch_texts, padding=True, truncation=True, max_length=MAX_LEN, return_tensors='pt').to(DEVICE)\n",
        "        with torch.no_grad():\n",
        "            logits_b = model_bert(enc_b['input_ids'], enc_b['attention_mask'])\n",
        "            prob_b = torch.sigmoid(logits_b)\n",
        "\n",
        "        # DeBERTa 예측\n",
        "        enc_d = tokenizer_d(batch_texts, padding=True, truncation=True, max_length=MAX_LEN, return_tensors='pt').to(DEVICE)\n",
        "        with torch.no_grad():\n",
        "            logits_d = model_deberta(enc_d['input_ids'], enc_d['attention_mask'])\n",
        "            prob_d = torch.sigmoid(logits_d)\n",
        "\n",
        "        # 앙상블 (DeBERTa 0.6 : BERT 0.4)\n",
        "        final_probs = (prob_b * 0.4 + prob_d * 0.6).cpu().numpy()\n",
        "\n",
        "        for probs in final_probs:\n",
        "            top_idx = probs.argsort()[-10:][::-1]\n",
        "            candidate_set = set()\n",
        "            score_map = {}\n",
        "\n",
        "            for cid in top_idx:\n",
        "                candidate_set.add(cid)\n",
        "                score_map[cid] = max(score_map.get(cid, 0), probs[cid])\n",
        "                curr_nodes = [cid]\n",
        "                while curr_nodes:\n",
        "                    next_nodes = []\n",
        "                    for node in curr_nodes:\n",
        "                        if node in parents_map:\n",
        "                            for p in parents_map[node]:\n",
        "                                candidate_set.add(p)\n",
        "                                score_map[p] = max(score_map.get(p, 0), probs[cid])\n",
        "                                next_nodes.append(p)\n",
        "                    curr_nodes = next_nodes\n",
        "\n",
        "            final_labels = sorted(list(candidate_set), key=lambda x: score_map[x], reverse=True)[:3]\n",
        "            if len(final_labels) < 2:\n",
        "                final_labels = probs.argsort()[-2:][::-1].tolist()\n",
        "            all_preds.append(sorted(final_labels))\n",
        "\n",
        "    with open(FINAL_CSV, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
        "        writer = csv.writer(f)\n",
        "        writer.writerow([\"id\", \"labels\"])\n",
        "        for pid, lbls in zip(test_pids, all_preds):\n",
        "            writer.writerow([pid, \",\".join(map(str, lbls))])\n",
        "    print(f\"✅ Success: {FINAL_CSV}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    run_ensemble_inference()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.14.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
